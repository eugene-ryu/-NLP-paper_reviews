{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Language Models are Few-Shot Learners<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAr0Qa%2FbtrbqvWPLwW%2FO4xjRi11B6VM5zKhxKEPBk%2Fimg.png)<br><br><br>\n",
        "- GPT2의 계승 모델로 GPT3라고 부른다<br>\n",
        "- GPT는 Generative Pre-Training의 약자 (GPT1 논문 제목이 Improving Language Understanding by Generative Pre-Training)<br><br>\n",
        "- input: N개의 word sequence<br>\n",
        "- output: N + 1번째의 단어<br><br>\n",
        "- GPT2 사이즈 업 + Unsupervised pre-training (like NLG) + Sparse Attention + No Fine Tuning<br><br><br>\n",
        "\n",
        "### Alternative dense and Locally banded sparse attention<br>\n",
        "- (a) Transformer처럼 앞쪽의 전부를 보면 연산량이 많으므로, (b)나 (c)처럼 제한된 개수의 input token에만 attention을 한다<br>\n",
        "- 긴 길이를 더 잘 attention 한다 (참고로, 사용한 seq_length (context window) = 2048)<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbPQZ9o%2Fbtrbva5lsfi%2FKAYeBY2mGUQZwiOElRfMo1%2Fimg.png)>br><br><br>\n",
        "\n",
        "- task-specific한 fune-tuning을 하지 않았다<br><br>\n",
        "- Autoregressive (AR) model (단방향 모델, uni-directional model)<br>\n",
        "- 175 billion (1750억) parameters를 가진 거대 모델<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FssPL0%2FbtrbtncXhQ0%2FwQU3NxRWVhhGdMT0TGSXk0%2Fimg.png)<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2RVMD%2Fbtrbtowbi55%2F0SqF1xTAwlPc9N1phZHDQK%2Fimg.png)<br><br><br>\n",
        "- Transformer에서 Decoder, 그 안에서도 Multi-head attention이 제거된 Decoder block으로 이루어져 있다<br><br><br>\n",
        "### BERT와 GPT 차이<br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbilueI%2FbtrbmxHY00A%2FYNy6CYiTl3kMAhvyLZCjHK%2Fimg.png)<br><br>\n",
        "![](https://blog.kakaocdn.net/dn/cCYEeF/btrbvaEa2D9/exYtCjxHKVKFY7KboOMakK/img.gif)<br><br><br><br>\n",
        "\n",
        "### Motivation<br>\n",
        "- 많은 unlabeled data로 task-agnostic한 model로 여러 NLP tasks를 해내자<br>\n",
        "(NLP의 각 task마다 데이터셋을 모으고, 힘들게 labeling을 하고, 모델을 pretrain + fine-[tuning하지 말고, fine-tuning하는 데에도 비용이 많이 드니)<br><br>\n",
        "- 논문의 전체적인 내용이 model size와 model performance간의 관계가 power-law를 따른다는 것을 증명해보이려고 함<br>\n",
        "- 단순히 모델의 크기를 키운 것 만으로도 (attention이 약간 다르긴 하지만) task-agnostic model이 충분히 가능하다는 것을 설명<br><br>\n",
        "### Mixtured Datasets<br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLb45r%2FbtrbvbJRpDz%2F88M5GokGZBvE5ungiBvV91%2Fimg.png)<br><br><br>\n",
        "- Common Crawl: 거의 trillion개의 words를 가진 거대 데이터셋, 그러나 퀄리티가 전반적으로 떨어져서 이를 올려주는 과정을 거쳤다.<br><br>\n",
        "1) original WebText (high-quality reference)를 Common Crawl 데이터셋과 분리하는 classifier를 훈련시킴<br>\n",
        "2) 1)의 classifier를 Common Crawl에 적용해서 resampling한 결과를 사용<br>\n",
        "3) document별로 hashs를 10개씩 달아서, 많이 겹치는 document는 삭제했다 (deduplication)<br>\n",
        "4) high-quality reference corpora (WebText, Books, Wikipedia)를 추가<br><br><br>\n",
        "\n",
        "- 데이터셋의 수 자체는 Common Crawl이 많지만, 좋은 quality data를 넣어줬을 때의 성능이 더 좋았기 때문에, 'Weight in training mix' 컬럼을 보면 수 자체는 더 적어도 high-quality인 데이터셋들이 힘을 쓸 수 있도록 mix를 구성했다<br><br>\n",
        "- 'Epochs elapsed when training for 300B tokens'를 보면, Common Crawl과 Books2는 각각 0.44, 0.43으로 각 데이터 전체를 쓰지 않았음을 알 수 있고<br>\n",
        "- 동시에 데이터셋 퀄리티가 상대적으로 높은 WebText2, Books1, Wikipedia는 각각 2.9, 1.9, 3.4로 여러번 데이터셋 전체를 돌렸음을 알 수 있다<br><br>\n",
        "- 거대 모델은 단순히 content를 기억해버리는 문제가 있어서, 이게 task를 contamination할 수 있으므로 overlaps를 최대한 없애려고 했으나, 버그 때문에 약간의 overlaps가 들어갔다<br><br>\n",
        "\n",
        "### Zero, One, Few-Shot Learning<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdun2V4%2FbtrbwgRHvH8%2FshKvlFA0Pa4ud2vkAZf8wk%2Fimg.png)<br><br>\n",
        "- Meta-learning: learning to learn, 학습 방법을 학습한다<br><br>\n",
        "- 2개의 loops를 확인할 수 있다. 한 개는 outer loop (gradient update가 된다), 한 개는 in-context learning이라 부르는 inner loop(gradient update가 안되기 때문에 실제 learning은 아니다)<br><br><br>\n",
        "- Fig. 1.1에서도 보이다시피 일관적인 input을 주면, 그 다음에 올 것을 model이 output으로 내는 형태, 즉, input자체가 task specification이 된다<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlIrhF%2FbtrbocJUNGP%2FKmIroIpVgTXJi7w7uZjuG1%2Fimg.png)<br><br><br>\n",
        "- Fine-tuning은 example을 주면서 지속적으로 gradient update를 하고, 결과를 낸다 (GPT3는 이것을 하지 않았다)>br><br>\n",
        "- GPT3의 approach는 Zero-Shot(0S) / One-Shot (1S) / Few-Shot Learning (FT)의 세 종류<br><br><br>\n",
        "- Zero-Shot : (Task description, prompt) 형태, 즉, (Translate English to French, cheese)로 들어간다.즉, model이 추론을 하는데에 어떤 힌트가 될 수 있는 아무런 example도 주어지지 않음<br><br>\n",
        "- One-Shot : (Task description, example, prompt) 형태, 단 하나의 example이 주어진다. 이때, 따로 example을 주지 않더라도, 내부적으로 들어가는 input은 모두 일관된 data<br><br>\n",
        "- Few-Shot : (Task description, example\"s\", prompt) 형태, 몇 개의 examples가 주어진다. examples 개수는, 10~100 사이에서 넣을 수 있는 만큼 (k=64, k=50.. 요런애들)<br>\n",
        "\n",
        "        - 장점 : task-specific data의 필요성이 대폭 감소된다 <br>\n",
        "        - 단점 : 여전히 일부는 small task-specific한 data가 필요하고, 여전히 fine-tuned SOTA에 비하면 성능이 떨어진다<br><br>\n",
        "\n",
        "- 굳이 \"one\"-shot을 zero나 few와 굳이 분리한 이유는, 예시 하나를 주면 나머지를 해내는 '사람간 커뮤니케이션과 비슷하기' 때문<br>\n",
        "- training 과정은 fill-in-the-blank로 이루어졌다. e.g.) Sally started to sing a song when Jack ____ => left<br>\n",
        "\n",
        "  예시에서 보이듯이 가장 마지막에 오는 것을 predict하는 형태<br><br>\n",
        "\n",
        "- Fine-tuning은 본 논문의 목적인 task-agnostic performance에 집중하기 위해서 하지 않았다 (future works)<br><br>\n",
        "\n",
        "## Performances<br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FCRBkI%2FbtrbpPuvqr2%2FZB4ywTKQ4gcNWkiE4gZkPk%2Fimg.png)<br><br>\n",
        "- 모델의 크기가 커질수록, 더 좋은 성능<br>\n",
        "- Zero-, One-보다 Few-Shot이 더 잘한다<br>\n",
        "- Natural language prompt는 task가 뭔지 알려줬다는 것이고 (e.g. add two numbers, 3+2 = ), no prompt는 task에 대해서 안 알려줬다는 것이다 (e.g. 3+2=)<br><br>\n",
        "- Zero-, One-에서는 prompt의 유무에 따라 성능차이가 좀 있는 걸 볼 수 있다<br>\n",
        "- Few-Shot에서도 처음엔 차이가 있었지만, 모델의 크기가 클수록 prompt의 유무와는 상관없이 잘 했다<br><br>\n",
        "- 이 모든 과정에서 gradient update도, fine-tuning도 없었다<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcXqYBg%2FbtrbqvvKMt7%2FyyY0ItcdVOQcyKJYkTeJGK%2Fimg.png)<br><br>\n",
        "- 모델 파라미터가 클수록 (x축), Zero-, One-, Few-Shot간 차이가 커지는 걸 볼 수 있다 (y축)<br><br>\n",
        "- 모델 size에 따라 performance가 달라지는 걸 확인하기 위해서, 8종류의 모델을 사용하였다<br><br>\n",
        "- 모든 모델의 context-window는 2048 tokens<br>\n",
        "- 모든 모델은 3000억개의 tokens를 학습한다<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FZTB8L%2Fbtrbwh38nU5%2Fj7bMSJCql2CKv20IbYANK1%2Fimg.png)<br><br>\n",
        "- n_params: trainable parameter의 전체 개수<br>\n",
        "- n_layers: 레이어의 개수<br>\n",
        "- d_model: 각 bottleneck 레이어 안에 있는 유닛 개수<br>\n",
        "- n_heads: self-attention에서 사용하는 head의 개수<br>\n",
        "- d_head: 각 head의 dimension<br><br>\n",
        "- optimizer는 Adam, cosine decay 사용, learning rate warmup 사용, weight decay는 0.1<br><br>\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FGnLV1%2Fbtrbh288smZ%2FMhprDIux1AjnX4oSurHkv0%2Fimg.png)<br><br>\n",
        "- Training 디테일: 모델 사이즈 가 크면, 배치 사이즈도 커야 하고, learning rate는 줄이자<br>\n",
        "- 배치사이즈는 Gradient noise로 guide를 해서 결정했다<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9sAGt%2FbtrbvcvfLzp%2FD9ypMckbfN6Dk44OAWJuT1%2Fimg.png)<br><br>\n",
        "- 8개 모델에 대한 훈련 곡선, parameters가 큰 모델(더 옅은색) 일수록 power-law 곡선을 벗어나는 모습을 보인다<br>\n",
        "- 퍼포먼스는 전체적으로 power-law 트렌드를 따른다 (특히 낮은 파라미터의 보라색 선을 보면 잘 보인다)<br><br>\n",
        "\n",
        "#### Power-law (멱함수의 법칙)?<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc8GKNm%2Fbtrbtyk7sYA%2FJAeM3cqHJ0skWZ9xkW3t6k%2Fimg.png)<br><br>\n",
        "specific한 test 결과로는 News Article Generation 결과를 가져왔습니다<br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHkM8d%2Fbtrbosl2qtF%2FR6um5K5WkMsJeoGWfBJTUk%2Fimg.png)<br><br>\n",
        "- 가장 큰 GPT3 175B의 결과를 보면 confidence interval이 49~54%로, 실제 사람이 쓴 것과 GPT3가 쓴 뉴스 기사를 반반의 확률로 사람이 구분해낼 수 있었음을 알 수 있다. 'I don't know'도 모델 크기에 따라 대체적으로 증가한 것을 볼 수 있다. (binary classification인데 1/2의 확률이므로 실제로는 랜덤)<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F3khAb%2FbtrbvbQDfl3%2FEkPToqB5LUQ5at02jpE4dk%2Fimg.png)<br><br>\n",
        "- 파라미터가 많을수록, loss가 상대적으로 더 낮음<br><br>\n",
        "\n",
        "### Limitations<br><br>\n",
        "성능이 많이 좋아졌지만, text synthesis와 몇몇 NLP tasks에서 취약점을 보였다<br><br>\n",
        "- text synthesis에서 동일한 말을 반복하거나, 내용이 길어지면 coherence(일관성)를 잃거나, 모순된 말을 하는 등의 문제<br>\n",
        "- 전문 dataset에 대해 좋은 결과를 내고도, common sense physics 질문에 취약한 모습을 보인다. 'If I put cheese into the fridge, will it melt(치즈를 냉장고에 넣으면, 녹을까?)'같은 질문에 고전했다<br><br>\n",
        "- AR이든 bidirectional 모델이든, 큰 모델이 가진 대체적인 문제가 제한적인 pretraining objective이다. 토큰별로 pretraining objective weight가 동일하게 주어짐에 따라, 무엇이 중요한지 무엇이 덜 중요한지를 잘 알지 못했다<br><br>\n",
        "- pre-training때 poor sample efficiency를 보이는 문제<br><br>\n",
        "- Few-Shot Learning에서 불명확한 부분이 있는데, 모델이 inference를 할 때, 바닥에서부터 new tasks를 배우는 것인지, 아니면 단순히 training때 배운 것을 기반으로 인식을 하는 것인지 알 수 없다<br><br>\n",
        "- Structural & Algorithmic limitation도 있는데, bidirectional의 이점을 포기했고, denoising같은 다른 training objectives도 포기했다. GPT3 사이즈의 bidirectional 모델, 더군다나 Few-, Zero-Shot Learning 식으로 해보면 좋을 것 같다. (future works)<br><br>\n",
        "- 보편적인 딥러닝 모델의 문제점인 decision을 해석하는게 쉽지 않은 문제가 여전히 있다<br><br>\n",
        "- (거의 모든 task에 일반적으로 adapt하기 위한 broad skills를 모델이 갖고 있기에) computational cost가 많이 들고 (강필성 교수님 동영상에 의하면 약 51억원), inference가 불편하다는 문제점<br><br>\n",
        "## Fairness, Bias, and Representation <br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdE7eaz%2Fbtrbtopr9mz%2FimP1KtFKgJ9QvyO6LWkwO0%2Fimg.png)<br><br>\n",
        "- 사용한 데이터셋에서 나타난 gender issue<br><br>\n",
        "- 각각 He was very _ / She was very _ 에서 _ 부분을 추측한 것<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbVocak%2FbtrbpPONX3G%2F7IhFXxuePi531K7kMUW2Vk%2Fimg.png)<br><br>\n",
        "- 인종차별적인 부분도 보였는데, Asian (파랑)은 무난한데 반해, Black (주황)에 대한 bad bias를 볼 수 있다<br>\n",
        "- 이외에 종교에 따른 차별도 있었다 (e.g. 기독교 키워드에 ignorant가 있거나, 유대교 키워드에 racist가 들어가는 등..)<br>\n",
        "- Energyu usage: 학습 시에는 많은 에너지가 필요한데, 학습 이후에는 약간의 에너지만을 사용하므로 효율적으로 사용할 수 있다<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQQSO7zEQ4sy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
