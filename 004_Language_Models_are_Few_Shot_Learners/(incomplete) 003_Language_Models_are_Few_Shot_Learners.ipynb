{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "## Language Models are Few-Shot Learners<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAr0Qa%2FbtrbqvWPLwW%2FO4xjRi11B6VM5zKhxKEPBk%2Fimg.png)<br><br><br>\n",
        "- GPT2의 계승 모델로 GPT3라고 부른다<br>\n",
        "- GPT는 Generative Pre-Training의 약자 (GPT1 논문 제목이 Improving Language Understanding by Generative Pre-Training)<br><br>\n",
        "- input: N개의 word sequence<br>\n",
        "- output: N + 1번째의 단어<br><br>\n",
        "- GPT2 사이즈 업 + Unsupervised pre-training (like NLG) + Sparse Attention + No Fine Tuning<br><br><br>\n",
        "\n",
        "### Alternative dense and Locally banded sparse attention<br>\n",
        "- (a) Transformer처럼 앞쪽의 전부를 보면 연산량이 많으므로, (b)나 (c)처럼 제한된 개수의 input token에만 attention을 한다<br>\n",
        "- 긴 길이를 더 잘 attention 한다 (참고로, 사용한 seq_length (context window) = 2048)<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbPQZ9o%2Fbtrbva5lsfi%2FKAYeBY2mGUQZwiOElRfMo1%2Fimg.png)>br><br><br>\n",
        "\n",
        "- task-specific한 fune-tuning을 하지 않았다<br><br>\n",
        "- Autoregressive (AR) model (단방향 모델, uni-directional model)<br>\n",
        "- 175 billion (1750억) parameters를 가진 거대 모델<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FssPL0%2FbtrbtncXhQ0%2FwQU3NxRWVhhGdMT0TGSXk0%2Fimg.png)<br><br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2RVMD%2Fbtrbtowbi55%2F0SqF1xTAwlPc9N1phZHDQK%2Fimg.png)<br><br><br>\n",
        "- Transformer에서 Decoder, 그 안에서도 Multi-head attention이 제거된 Decoder block으로 이루어져 있다<br><br><br>\n",
        "### BERT와 GPT 차이<br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbilueI%2FbtrbmxHY00A%2FYNy6CYiTl3kMAhvyLZCjHK%2Fimg.png)<br><br>\n",
        "![](https://blog.kakaocdn.net/dn/cCYEeF/btrbvaEa2D9/exYtCjxHKVKFY7KboOMakK/img.gif)<br><br><br><br>\n",
        "\n",
        "### Motivation<br>\n",
        "- 많은 unlabeled data로 task-agnostic한 model로 여러 NLP tasks를 해내자<br>\n",
        "(NLP의 각 task마다 데이터셋을 모으고, 힘들게 labeling을 하고, 모델을 pretrain + fine-[tuning하지 말고, fine-tuning하는 데에도 비용이 많이 드니)<br><br>\n",
        "- 논문의 전체적인 내용이 model size와 model performance간의 관계가 power-law를 따른다는 것을 증명해보이려고 함<br>\n",
        "- 단순히 모델의 크기를 키운 것 만으로도 (attention이 약간 다르긴 하지만) task-agnostic model이 충분히 가능하다는 것을 설명<br><br>\n",
        "### Mixtured Datasets<br>\n",
        "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLb45r%2FbtrbvbJRpDz%2F88M5GokGZBvE5ungiBvV91%2Fimg.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQQSO7zEQ4sy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}